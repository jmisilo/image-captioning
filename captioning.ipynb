{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d34befa",
   "metadata": {},
   "source": [
    "ðŸ“– **Resources**:\n",
    " * COCO - https://cocodataset.org/\n",
    " * EfficientNet B6 - https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b6.html#torchvision.models.efficientnet_b6\n",
    " * T5 Model HF - https://huggingface.co/docs/transformers/model_doc/t5#t5#\n",
    " * T5 Small HF - https://huggingface.co/t5-small\n",
    " * Aladdin Persson's image captioning repo https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/more_advanced/image_captioning\n",
    " * Guide to image captioning - https://towardsdatascience.com/a-guide-to-image-captioning-e9fd5517f350\n",
    " * Andrej Karpathy's Deep Visual-Semantic Alignments for Generating Image Descriptions - https://cs.stanford.edu/people/karpathy/deepimagesent/\n",
    " * Andrej Karpathy's minGPT implementation https://github.com/karpathy/minGPT\n",
    " * Aladdin Persson's transformer from scratch - https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/more_advanced/transformer_from_scratch\n",
    " * The role of the mask in transformer's decoder - https://ai.stackexchange.com/questions/23889/what-is-the-purpose-of-decoder-mask-triangular-mask-in-transformer\n",
    " * Semi-Autoregressive Transformer for Image Captioning (Relaxed mask idea) - https://arxiv.org/pdf/2106.09436.pdf\n",
    " * Transformer guide (training + evaluating) https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c5df2",
   "metadata": {},
   "source": [
    "ðŸ”‘ **Note**: Images from Coco will be downloaded, because model will be trained using GoogleColab, drive if which allow us to upload ~15 GB of data. Coco dataset is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f038cc4",
   "metadata": {},
   "source": [
    "## 1. Import depedencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bbbb7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from skimage.transform import resize\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b24c0d6",
   "metadata": {},
   "source": [
    "## 2. Data expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0417a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for expectations\n",
    "with open(os.path.join('data', 'annotations', 'captions_train2017.json'), 'r') as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3ce217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'http://creativecommons.org/licenses/by-nc-sa/2.0/',\n",
       "  'id': 1,\n",
       "  'name': 'Attribution-NonCommercial-ShareAlike License'},\n",
       " {'url': 'http://creativecommons.org/licenses/by-nc/2.0/',\n",
       "  'id': 2,\n",
       "  'name': 'Attribution-NonCommercial License'},\n",
       " {'url': 'http://creativecommons.org/licenses/by-nc-nd/2.0/',\n",
       "  'id': 3,\n",
       "  'name': 'Attribution-NonCommercial-NoDerivs License'},\n",
       " {'url': 'http://creativecommons.org/licenses/by/2.0/',\n",
       "  'id': 4,\n",
       "  'name': 'Attribution License'},\n",
       " {'url': 'http://creativecommons.org/licenses/by-sa/2.0/',\n",
       "  'id': 5,\n",
       "  'name': 'Attribution-ShareAlike License'},\n",
       " {'url': 'http://creativecommons.org/licenses/by-nd/2.0/',\n",
       "  'id': 6,\n",
       "  'name': 'Attribution-NoDerivs License'},\n",
       " {'url': 'http://flickr.com/commons/usage/',\n",
       "  'id': 7,\n",
       "  'name': 'No known copyright restrictions'},\n",
       " {'url': 'http://www.usa.gov/copyright.shtml',\n",
       "  'id': 8,\n",
       "  'name': 'United States Government Work'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['licenses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15bc836e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'COCO 2017 Dataset',\n",
       " 'url': 'http://cocodataset.org',\n",
       " 'version': '1.0',\n",
       " 'year': 2017,\n",
       " 'contributor': 'COCO Consortium',\n",
       " 'date_created': '2017/09/01'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1730ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>license</th>\n",
       "      <th>file_name</th>\n",
       "      <th>coco_url</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>date_captured</th>\n",
       "      <th>flickr_url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>000000391895.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>360</td>\n",
       "      <td>640</td>\n",
       "      <td>2013-11-14 11:18:45</td>\n",
       "      <td>http://farm9.staticflickr.com/8186/8119368305_...</td>\n",
       "      <td>391895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>000000522418.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>2013-11-14 11:38:44</td>\n",
       "      <td>http://farm1.staticflickr.com/1/127244861_ab0c...</td>\n",
       "      <td>522418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>000000184613.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>336</td>\n",
       "      <td>500</td>\n",
       "      <td>2013-11-14 12:36:29</td>\n",
       "      <td>http://farm3.staticflickr.com/2169/2118578392_...</td>\n",
       "      <td>184613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000000318219.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>640</td>\n",
       "      <td>556</td>\n",
       "      <td>2013-11-14 13:02:53</td>\n",
       "      <td>http://farm5.staticflickr.com/4125/5094763076_...</td>\n",
       "      <td>318219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>000000554625.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>640</td>\n",
       "      <td>426</td>\n",
       "      <td>2013-11-14 16:03:19</td>\n",
       "      <td>http://farm5.staticflickr.com/4086/5094162993_...</td>\n",
       "      <td>554625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118282</th>\n",
       "      <td>1</td>\n",
       "      <td>000000444010.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>2013-11-25 14:46:11</td>\n",
       "      <td>http://farm4.staticflickr.com/3697/9303670993_...</td>\n",
       "      <td>444010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118283</th>\n",
       "      <td>3</td>\n",
       "      <td>000000565004.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>427</td>\n",
       "      <td>640</td>\n",
       "      <td>2013-11-25 19:59:30</td>\n",
       "      <td>http://farm2.staticflickr.com/1278/4677568591_...</td>\n",
       "      <td>565004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118284</th>\n",
       "      <td>3</td>\n",
       "      <td>000000516168.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>480</td>\n",
       "      <td>640</td>\n",
       "      <td>2013-11-25 21:03:34</td>\n",
       "      <td>http://farm3.staticflickr.com/2379/2293730995_...</td>\n",
       "      <td>516168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118285</th>\n",
       "      <td>4</td>\n",
       "      <td>000000547503.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>375</td>\n",
       "      <td>500</td>\n",
       "      <td>2013-11-25 21:20:21</td>\n",
       "      <td>http://farm1.staticflickr.com/178/423174638_1c...</td>\n",
       "      <td>547503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118286</th>\n",
       "      <td>4</td>\n",
       "      <td>000000475546.jpg</td>\n",
       "      <td>http://images.cocodataset.org/train2017/000000...</td>\n",
       "      <td>375</td>\n",
       "      <td>500</td>\n",
       "      <td>2013-11-25 21:20:23</td>\n",
       "      <td>http://farm1.staticflickr.com/167/423175046_6c...</td>\n",
       "      <td>475546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>118287 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        license         file_name  \\\n",
       "0             3  000000391895.jpg   \n",
       "1             4  000000522418.jpg   \n",
       "2             3  000000184613.jpg   \n",
       "3             3  000000318219.jpg   \n",
       "4             3  000000554625.jpg   \n",
       "...         ...               ...   \n",
       "118282        1  000000444010.jpg   \n",
       "118283        3  000000565004.jpg   \n",
       "118284        3  000000516168.jpg   \n",
       "118285        4  000000547503.jpg   \n",
       "118286        4  000000475546.jpg   \n",
       "\n",
       "                                                 coco_url  height  width  \\\n",
       "0       http://images.cocodataset.org/train2017/000000...     360    640   \n",
       "1       http://images.cocodataset.org/train2017/000000...     480    640   \n",
       "2       http://images.cocodataset.org/train2017/000000...     336    500   \n",
       "3       http://images.cocodataset.org/train2017/000000...     640    556   \n",
       "4       http://images.cocodataset.org/train2017/000000...     640    426   \n",
       "...                                                   ...     ...    ...   \n",
       "118282  http://images.cocodataset.org/train2017/000000...     480    640   \n",
       "118283  http://images.cocodataset.org/train2017/000000...     427    640   \n",
       "118284  http://images.cocodataset.org/train2017/000000...     480    640   \n",
       "118285  http://images.cocodataset.org/train2017/000000...     375    500   \n",
       "118286  http://images.cocodataset.org/train2017/000000...     375    500   \n",
       "\n",
       "              date_captured  \\\n",
       "0       2013-11-14 11:18:45   \n",
       "1       2013-11-14 11:38:44   \n",
       "2       2013-11-14 12:36:29   \n",
       "3       2013-11-14 13:02:53   \n",
       "4       2013-11-14 16:03:19   \n",
       "...                     ...   \n",
       "118282  2013-11-25 14:46:11   \n",
       "118283  2013-11-25 19:59:30   \n",
       "118284  2013-11-25 21:03:34   \n",
       "118285  2013-11-25 21:20:21   \n",
       "118286  2013-11-25 21:20:23   \n",
       "\n",
       "                                               flickr_url      id  \n",
       "0       http://farm9.staticflickr.com/8186/8119368305_...  391895  \n",
       "1       http://farm1.staticflickr.com/1/127244861_ab0c...  522418  \n",
       "2       http://farm3.staticflickr.com/2169/2118578392_...  184613  \n",
       "3       http://farm5.staticflickr.com/4125/5094763076_...  318219  \n",
       "4       http://farm5.staticflickr.com/4086/5094162993_...  554625  \n",
       "...                                                   ...     ...  \n",
       "118282  http://farm4.staticflickr.com/3697/9303670993_...  444010  \n",
       "118283  http://farm2.staticflickr.com/1278/4677568591_...  565004  \n",
       "118284  http://farm3.staticflickr.com/2379/2293730995_...  516168  \n",
       "118285  http://farm1.staticflickr.com/178/423174638_1c...  547503  \n",
       "118286  http://farm1.staticflickr.com/167/423175046_6c...  475546  \n",
       "\n",
       "[118287 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f117681f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203564</td>\n",
       "      <td>37</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>322141</td>\n",
       "      <td>49</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16977</td>\n",
       "      <td>89</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106140</td>\n",
       "      <td>98</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106140</td>\n",
       "      <td>101</td>\n",
       "      <td>There is a GOL plane taking off in a partly cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591748</th>\n",
       "      <td>133071</td>\n",
       "      <td>829655</td>\n",
       "      <td>a slice of bread is covered with a sour cream ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591749</th>\n",
       "      <td>410182</td>\n",
       "      <td>829658</td>\n",
       "      <td>A long plate hold some fries with some sliders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591750</th>\n",
       "      <td>180285</td>\n",
       "      <td>829665</td>\n",
       "      <td>Two women sit and pose with stuffed animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591751</th>\n",
       "      <td>133071</td>\n",
       "      <td>829693</td>\n",
       "      <td>White Plate with a lot of guacamole and an ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591752</th>\n",
       "      <td>133071</td>\n",
       "      <td>829717</td>\n",
       "      <td>A dinner plate has a lemon wedge garnishment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591753 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id      id                                            caption\n",
       "0         203564      37  A bicycle replica with a clock as the front wh...\n",
       "1         322141      49  A room with blue walls and a white sink and door.\n",
       "2          16977      89  A car that seems to be parked illegally behind...\n",
       "3         106140      98  A large passenger airplane flying through the ...\n",
       "4         106140     101  There is a GOL plane taking off in a partly cl...\n",
       "...          ...     ...                                                ...\n",
       "591748    133071  829655  a slice of bread is covered with a sour cream ...\n",
       "591749    410182  829658  A long plate hold some fries with some sliders...\n",
       "591750    180285  829665       Two women sit and pose with stuffed animals.\n",
       "591751    133071  829693  White Plate with a lot of guacamole and an ext...\n",
       "591752    133071  829717      A dinner plate has a lemon wedge garnishment.\n",
       "\n",
       "[591753 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['annotations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e42edf",
   "metadata": {},
   "source": [
    "ðŸ”‘ **Note**: As we can see we don't need every column from data, so we can drop some. In fact we can only use annotations and generate url for img using id.\n",
    "ðŸ”‘ **Note**: Dataset will contain images and five captions for each, because downloading each images five times takes too much time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e7b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['info']\n",
    "del data['licenses']\n",
    "del data['images']\n",
    "\n",
    "data = pd.DataFrame(data['annotations']).drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f97d36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>203564</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>322141</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16977</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>106140</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>106140</td>\n",
       "      <td>There is a GOL plane taking off in a partly cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591748</th>\n",
       "      <td>133071</td>\n",
       "      <td>a slice of bread is covered with a sour cream ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591749</th>\n",
       "      <td>410182</td>\n",
       "      <td>A long plate hold some fries with some sliders...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591750</th>\n",
       "      <td>180285</td>\n",
       "      <td>Two women sit and pose with stuffed animals.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591751</th>\n",
       "      <td>133071</td>\n",
       "      <td>White Plate with a lot of guacamole and an ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591752</th>\n",
       "      <td>133071</td>\n",
       "      <td>A dinner plate has a lemon wedge garnishment.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>591753 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_id                                            caption\n",
       "0         203564  A bicycle replica with a clock as the front wh...\n",
       "1         322141  A room with blue walls and a white sink and door.\n",
       "2          16977  A car that seems to be parked illegally behind...\n",
       "3         106140  A large passenger airplane flying through the ...\n",
       "4         106140  There is a GOL plane taking off in a partly cl...\n",
       "...          ...                                                ...\n",
       "591748    133071  a slice of bread is covered with a sour cream ...\n",
       "591749    410182  A long plate hold some fries with some sliders...\n",
       "591750    180285       Two women sit and pose with stuffed animals.\n",
       "591751    133071  White Plate with a lot of guacamole and an ext...\n",
       "591752    133071      A dinner plate has a lemon wedge garnishment.\n",
       "\n",
       "[591753 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3be9c",
   "metadata": {},
   "source": [
    "**Comparision number of samples with limited and unlimited length and their ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05e82492",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = data.apply(lambda x: len(x['caption'].split(' ')), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69f28ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(lens, 98.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdc4ea95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581751, 591753, 0.9830976775783139)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lens[lens < 18]), len(lens), len(lens[lens < 18]) / len(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a7bc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32b5db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class cfg:\n",
    "    epochs = 5\n",
    "    batch_size = 12 # 5*12 captions\n",
    "    lr = 1e-5\n",
    "    max_len = 18\n",
    "    width = 489\n",
    "    height = 456\n",
    "    embed_size = 128\n",
    "    num_layers = 6\n",
    "    num_heads = 8\n",
    "    forward_expansion = 4\n",
    "    dropout = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2966a8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vocab = {\n",
    "            '<unk>': 0,\n",
    "            '<pad>': 1,\n",
    "            '<sos>': 2,\n",
    "            '<eos>': 3\n",
    "        }\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert type(index) in [str, int], 'Index type must be string or int'\n",
    "        \n",
    "        if isinstance(index, str):\n",
    "            try:\n",
    "                return self.vocab[index]\n",
    "            \n",
    "            except KeyError:\n",
    "                return self.vocab['<unk>']\n",
    "        \n",
    "        elif isinstance(index, int):\n",
    "            try:\n",
    "                return list(self.vocab.keys())[list(self.vocab.values()).index(index)]\n",
    "            except (KeyError,ValueError):\n",
    "                return self[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def append_word(self, word):\n",
    "        if not word in self.vocab:\n",
    "            self.vocab[word] = len(self)\n",
    "    \n",
    "    def build_vocab(self, data):\n",
    "        \"\"\"\n",
    "            Takes array-like object.\n",
    "        \"\"\"\n",
    "        \n",
    "        for _ in range(2):\n",
    "            data = list(itertools.chain.from_iterable(data))\n",
    "        \n",
    "        bag_of_words = sorted(list(set(data)))\n",
    "        \n",
    "        for word in bag_of_words:\n",
    "            self.append_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86e7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptions(Dataset):\n",
    "    def __init__(self, path):\n",
    "        \"\"\"\n",
    "            path: train/valid\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(os.path.join('data', 'annotations', f'captions_{path}2017.json'), 'r') as f:\n",
    "            self._data = json.loads(f.read())\n",
    "        \n",
    "        self.path = path\n",
    "        \n",
    "        self._data = pd.DataFrame(self._data['annotations']).drop('id', axis=1)\n",
    "        \n",
    "        self.split_data()\n",
    "        \n",
    "        self.preprocessing()\n",
    "        \n",
    "        self.vocab = Vocabulary()\n",
    "\n",
    "        self.vocab.build_vocab(self._data.values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self._data.iloc[index]\n",
    "\n",
    "        url = self.id_to_url(int(data.name))\n",
    "        \n",
    "        img = io.imread(url)\n",
    "        img = resize(img, (cfg.height, cfg.width), anti_aliasing=True)\n",
    "        \n",
    "        caps = []\n",
    "        for cap in data.values:\n",
    "            caps.append([self.vocab[word] for word in cap])\n",
    "        \n",
    "        return img, caps\n",
    "        \n",
    "    def split_data(self):\n",
    "        ids = self._data['image_id'].value_counts() \n",
    "        d = []\n",
    "        \n",
    "        for ind in list(set(list(ids[ids == 5].index))):\n",
    "            s = self._data[self._data['image_id'] == ind]['caption'].tolist()\n",
    "            d.append([ind, *s])\n",
    "        \n",
    "        df = pd.DataFrame(d, columns=['image_id', *[f'cap-{i}' for i in range(5)]]).set_index('image_id')\n",
    "        \n",
    "        self._data = df\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        # 5 because we have 5 captions per image \n",
    "        for i in range(5):\n",
    "                                                            # lowercase -> remove punctuations -> split \n",
    "            self._data[f'cap-{i}'] = self._data[f'cap-{i}'].apply(lambda x: x.lower().translate(str.maketrans('', '', string.punctuation)).split(' '))\n",
    "        \n",
    "    def id_to_url(self, id_):\n",
    "        length_id = 12\n",
    "        id_ = str(id_)\n",
    "        id_str = ''.join(['0' for _ in range(length_id - len(id_))]) + id_\n",
    "\n",
    "        return f'http://images.cocodataset.org/{self.path}2017/{id_str}.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899d8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CocoCaptions('val')\n",
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8a2f2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code to adjust for new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0799c529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(batch):\n",
    "    \n",
    "    imgs = []\n",
    "    captions = []\n",
    "    \n",
    "    for img, caps in batch:\n",
    "        \n",
    "        caps_pp = []\n",
    "        for cap in caps:\n",
    "            cap = [vocab['<sos>'], *cap]\n",
    "            text_len = len(cap)\n",
    "\n",
    "            if text_len >= (cfg.max_len + 1):\n",
    "                cap = cap[:cfg.max_len+1]\n",
    "                cap[-1] = vocab['<eos>']    \n",
    "\n",
    "            else:\n",
    "                cap.append(vocab['<eos>'])\n",
    "                pad_len = cfg.max_len - (text_len)\n",
    "                \n",
    "                for i in range(pad_len):\n",
    "                    cap.append(data.vocab['<pad>'])\n",
    "                    \n",
    "            caps_pp.append(torch.IntTensor(cap))\n",
    "            \n",
    "        captions.append(torch.stack(caps_pp).type(torch.int64))\n",
    "        imgs.append(torch.Tensor(img))\n",
    "\n",
    "    return torch.stack(imgs), torch.stack(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51739970",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(data, batch_size=2, collate_fn=pad_seq, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c58a9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in train_loader:\n",
    "#     plt.imshow(i[0][0])\n",
    "#     plt.show()\n",
    "    \n",
    "#     print('--CAPTIONS--')\n",
    "#     for j in i[1][0]:\n",
    "#         print(' '.join([vocab[word.item()] for word in j]))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2f4048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(2048, 32*20)\n",
    "        self.c1 = nn.Conv1d(32, embed_size, kernel_size=3, stride=2, padding=6)\n",
    "        self.c2 = nn.Conv1d(embed_size, embed_size, kernel_size=2, stride=1, padding=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: N, 2048\n",
    "        x = F.gelu(self.fc(x))\n",
    "        \n",
    "        x = x.view(-1, 32, 20)\n",
    "\n",
    "#         x = x.permute(0, 2, 1)\n",
    "        x = F.gelu(self.c1(x))\n",
    "        \n",
    "        x = self.c2(x)\n",
    "        return x.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "683af414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(nn.Module):\n",
    "    def __init__(self, max_length):\n",
    "        super(EffNet, self).__init__()\n",
    "        \n",
    "        self.eff = models.efficientnet_b5(pretrained=True, progress=True)\n",
    "        self.eff.classifier = Classifier(128) # nn.Linear(2048, max_length)\n",
    "        \n",
    "        self._freeze_layers()\n",
    "        \n",
    "    def _freeze_layers(self):\n",
    "        for mod in self.eff.features[:-2].parameters():\n",
    "            mod.requires_grad = False\n",
    "    \n",
    "    def forward(self, img):\n",
    "        x = self.eff(img)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7e96970",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "        \n",
    "        assert self.head_dim * heads == embed_size, 'Embed size needs to be divisible by heads'\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        \n",
    "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size)\n",
    "        \n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "        \n",
    "        values = self.values(values) \n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "        \n",
    "        energy = torch.einsum('nqhd, nkhd->nhqk', [queries, keys])\n",
    "        \n",
    "        if mask is not None:\n",
    "            # if mask at same point is 0 - shitdown this point - set to -inf, in softmax it will be 0\n",
    "            energy = energy.masked_fill(mask == 0, -1e20)\n",
    "        \n",
    "        attention = torch.softmax(energy / (self.embed_size**(1/2)), dim=3)\n",
    "        \n",
    "        # attention shape: N, heads, query_len, key_len\n",
    "        # values shape: N, value_len, heads, head_dim\n",
    "        # out shape: N, query_len, heads, head_dim\n",
    "        out = torch.einsum('nhql, nlhd->nqhd', [attention, values])\n",
    "        \n",
    "        out = out.reshape(N, query_len, self.heads*self.head_dim)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a314e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion*embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion*embed_size, embed_size)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, value, key, queries, mask):\n",
    "        attention = self.attention(value, key, queries, mask)\n",
    "        \n",
    "        x = self.dropout(self.norm1(attention + queries))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ad58cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, None)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0690f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for module in self.named_parameters():\n",
    "            if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "                module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "                if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "                    \n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "                \n",
    "    def forward(self, x, enc_out, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48639735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioner(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_len, num_layers, num_heads, forward_expansion, dropout, device):\n",
    "        super(ImageCaptioner, self).__init__()\n",
    "        \n",
    "        self.eff_enc = EffNet(max_len)\n",
    "        self.trans_dec = Decoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            heads=num_heads,\n",
    "            forward_expansion=forward_expansion,\n",
    "            dropout=dropout,\n",
    "            device=device,\n",
    "            max_length=max_len\n",
    "        )\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(par.numel() for par in self.parameters())\n",
    "    \n",
    "    def make_mask(self, trg):\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        )\n",
    "\n",
    "        return trg_mask.to(device)\n",
    "\n",
    "    def forward(self, img, trg):\n",
    "        enc_out = self.eff_enc(img.permute(0, 3, 2, 1))\n",
    "        \n",
    "        dec_out = self.trans_dec(trg, enc_out, self.make_mask(trg))\n",
    "        \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca78f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68633645",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioner(\n",
    "            vocab_size=len(vocab),\n",
    "            embed_size=cfg.embed_size,\n",
    "            num_layers=cfg.num_layers,\n",
    "            num_heads=cfg.num_heads,\n",
    "            forward_expansion=cfg.forward_expansion,\n",
    "            dropout=cfg.dropout,\n",
    "            device=device,\n",
    "            max_len=cfg.max_len\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97abfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ae72b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, criterion, loader):\n",
    "    running_loss = []\n",
    "\n",
    "    t0 = time.time()\n",
    "    for b_idx, (img, trgs) in enumerate(loader):\n",
    "        img = img.to(device)\n",
    "        trgs = trgs.to(device)\n",
    "        \n",
    "        for trg in trgs.permute(1, 0, 2):\n",
    "            plt.imshow(img[0])\n",
    "            \n",
    "            scores = model(img, trg[:, :-1])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(scores.permute(0, 2, 1), trg[:, 1:])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "    loss = sum(running_loss) / len(running_loss)\n",
    "\n",
    "    print(f'Loss: {loss}, time: {time.time() - t0}s')\n",
    "    return sum(running_loss) / len(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8cda596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "loss = []\n",
    "\n",
    "for epoch in range(cfg.epochs):\n",
    "    pass\n",
    "    loss.append(train_epoch(model, optimizer, criterion, train_loader))\n",
    "    \n",
    "plt.plot(loss)\n",
    "plt.show()\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join('drive', 'MyDrive', 'Colab Notebooks', 'models', 'captioner-1.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-captioning",
   "language": "python",
   "name": "image-captioning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
